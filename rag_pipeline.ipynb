{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:11.494018Z",
     "start_time": "2025-09-14T00:36:10.824609Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç—å–∏\n",
    "url = \"https://blog.dzencode.com/ru/illyuziya-kachestva-vash-sayt-idealen-pozdravlyaem-vy-tolko-chto-sozhgli-byudzhet/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "article_text = soup.get_text(separator='\\n')\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞\n",
    "with open('artifacts/article.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(article_text)\n",
    "\n",
    "# –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ\n",
    "metadata = {\n",
    "    \"url\": url,\n",
    "    \"language\": \"ru\",\n",
    "    \"date\": \"unknown\",  # –î–∞—Ç–∞ –Ω–µ —É–∫–∞–∑–∞–Ω–∞, –º–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ–∑–∂–µ\n",
    "    \"topic\": \"–ò–ª–ª—é–∑–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –≤–µ–±-–¥–∏–∑–∞–π–Ω–µ\",\n",
    "    \"project\": \"RAG_Pipeline_Test\",\n",
    "    \"lang\": \"ru\"\n",
    "}\n",
    "with open('artifacts/metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)"
   ],
   "id": "cd7aaf2855bf1a94",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:11.523636Z",
     "start_time": "2025-09-14T00:36:11.509757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "with open('artifacts/article.txt', 'r', encoding='utf-8') as f:\n",
    "    article_text = f.read()\n",
    "\n",
    "# –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: —É–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤ –∏ –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤\n",
    "cleaned_text = re.sub(r'<.*?>', '', article_text)  # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤\n",
    "cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)  # –ó–∞–º–µ–Ω–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–∞ –æ–¥–∏–Ω\n",
    "cleaned_lines = cleaned_text.splitlines()  # –†–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ –ª—é–±—ã–º —Ä–∞–∑—Ä—ã–≤–∞–º —Å—Ç—Ä–æ–∫\n",
    "cleaned_text = '\\n'.join(line.strip() for line in cleaned_lines if line.strip())  # –§–∏–ª—å—Ç—Ä –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "with open('artifacts/cleaned_article.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)"
   ],
   "id": "eb45dfd05873e40d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:12.875353Z",
     "start_time": "2025-09-14T00:36:11.692380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "with open('artifacts/cleaned_article.txt', 'r', encoding='utf-8') as f:\n",
    "    cleaned_text = f.read()\n",
    "\n",
    "# –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "chunks = text_splitter.split_text(cleaned_text)\n",
    "\n",
    "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è JSONL —Å ID –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏\n",
    "data = []\n",
    "for i, chunk in enumerate(chunks):\n",
    "    entry = {\n",
    "        \"id\": i,\n",
    "        \"text\": chunk,\n",
    "        \"metadata\": {\n",
    "            \"url\": \"https://blog.dzencode.com/ru/illyuziya-kachestva-vash-sayt-idealen-pozdravlyaem-vy-tolko-chto-sozhgli-byudzhet/\",\n",
    "            \"language\": \"ru\",\n",
    "            \"date\": \"unknown\",\n",
    "            \"topic\": \"–ò–ª–ª—é–∑–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≤ –≤–µ–±-–¥–∏–∑–∞–π–Ω–µ\",\n",
    "            \"project\": \"RAG_Pipeline_Test\"\n",
    "        }\n",
    "    }\n",
    "    data.append(entry)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSONL\n",
    "with open('artifacts/rag_article.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for entry in data:\n",
    "        f.write(json.dumps(entry, ensure_ascii=False) + '\\n')"
   ],
   "id": "bb5f229f475b136c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:23.221818Z",
     "start_time": "2025-09-14T00:36:12.882517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import VectorParams, Distance, PointStruct\n",
    "import json\n",
    "import pickle\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# –ß—Ç–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤ –∏–∑ JSONL\n",
    "chunks = []\n",
    "try:\n",
    "    with open('artifacts/rag_article.jsonl', 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            chunks.append(json.loads(line))\n",
    "except FileNotFoundError:\n",
    "    print(\"–û—à–∏–±–∫–∞: –§–∞–π–ª 'artifacts/rag_article.jsonl' –Ω–µ –Ω–∞–π–¥–µ–Ω.\")\n",
    "    raise\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "texts = [chunk['text'] if chunk['text'] else \" \" for chunk in chunks]\n",
    "vectors = embeddings.embed_documents(texts)\n",
    "\n",
    "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant\n",
    "try:\n",
    "    client = QdrantClient(host='localhost', port=6333)\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Qdrant: {e}\")\n",
    "    raise\n",
    "\n",
    "# –û—á–∏—Å—Ç–∫–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n",
    "collection_name = \"rag_article_collection\"\n",
    "if client.collection_exists(collection_name):\n",
    "    client.delete_collection(collection_name)\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=len(vectors[0]),\n",
    "        distance=Distance.COSINE\n",
    "    )\n",
    ")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ Qdrant\n",
    "points = [\n",
    "    PointStruct(\n",
    "        id=chunk['id'],\n",
    "        vector=vector,\n",
    "        payload=chunk['metadata']\n",
    "    )\n",
    "    for chunk, vector in zip(chunks, vectors)\n",
    "]\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "with open('artifacts/embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(vectors, f)"
   ],
   "id": "f96ac98ac3ebe6ea",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\balnc3r\\AppData\\Local\\Temp\\ipykernel_16204\\3254091620.py:8: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:25.015338Z",
     "start_time": "2025-09-14T00:36:23.237881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "MODEL_NAME = \"facebook/mbart-large-50-many-to-many-mmt\"\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, src_lang=\"ru_RU\", use_fast=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "    print(f\"–ú–æ–¥–µ–ª—å {MODEL_NAME} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
    "except Exception as e:\n",
    "    print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {MODEL_NAME}: {e}\")\n",
    "    raise"
   ],
   "id": "11e5587e2e57e3f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ú–æ–¥–µ–ª—å facebook/mbart-large-50-many-to-many-mmt —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-14T00:36:36.414815Z",
     "start_time": "2025-09-14T00:36:25.030081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "import mlflow  # –ù–æ–≤—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è MLflow\n",
    "\n",
    "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant\n",
    "client = QdrantClient(host='localhost', port=6333)\n",
    "collection_name = \"rag_article_collection\"\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ JSONL\n",
    "chunks = []\n",
    "with open('artifacts/rag_article.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        chunks.append(json.loads(line))\n",
    "\n",
    "# –°–ª–æ–≤–∞—Ä—å –¥–ª—è –¥–æ—Å—Ç—É–ø–∞ –∫ —Ç–µ–∫—Å—Ç—É –ø–æ id\n",
    "chunk_dict = {chunk['id']: chunk['text'] for chunk in chunks}\n",
    "\n",
    "\n",
    "# –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤\n",
    "def search_chunks(query, top_k=3, min_score=0.4):\n",
    "    query_vector = embeddings.embed_query(query)\n",
    "    search_result = client.query_points(\n",
    "        collection_name=collection_name,\n",
    "        query=query_vector,\n",
    "        limit=top_k,\n",
    "        with_payload=True,\n",
    "        with_vectors=False\n",
    "    ).points\n",
    "    matched_chunks = [\n",
    "        chunk_dict[hit.id] for hit in search_result\n",
    "        if hit.id in chunk_dict and hit.score >= min_score\n",
    "    ]\n",
    "    return matched_chunks\n",
    "\n",
    "\n",
    "# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞\n",
    "def generate_response(query, chunks):\n",
    "    if not chunks:\n",
    "        return \"–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.\"\n",
    "    context = chunks[0]\n",
    "    tokenized_context = tokenizer(context, truncation=True, max_length=150, return_tensors=\"pt\")\n",
    "    truncated_context = tokenizer.decode(tokenized_context[\"input_ids\"][0], skip_special_tokens=True)\n",
    "    prompt = f\"\"\"–û—Ç–≤–µ—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –∫—Ä–∞—Ç–∫–æ, –∏—Å–ø–æ–ª—å–∑—É—è –∫–æ–Ω—Ç–µ–∫—Å—Ç.\n",
    "\n",
    "–í–æ–ø—Ä–æ—Å: {query}\n",
    "–ö–æ–Ω—Ç–µ–∫—Å—Ç: {truncated_context}\"\"\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=150,\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"ru_RU\"]\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏\n",
    "collection_info = client.get_collection(collection_name=collection_name)\n",
    "if collection_info.points_count == 0 or collection_info.points_count != len(chunk_dict):\n",
    "    raise SystemExit\n",
    "\n",
    "\n",
    "# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –æ—Ç–≤–µ—Ç–∞\n",
    "def log_query(query, answer, chunks):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"query\": query,\n",
    "        \"answer\": answer,\n",
    "        \"relevant_chunks\": [chunk[:100] + \"...\" if chunk else \"–ü—É—Å—Ç–æ\" for chunk in chunks]\n",
    "    }\n",
    "    os.makedirs('logs', exist_ok=True)\n",
    "    log_file = 'logs/query_log.json'\n",
    "    try:\n",
    "        with open(log_file, 'r', encoding='utf-8') as f:\n",
    "            logs = json.load(f)\n",
    "    except (FileNotFoundError, json.JSONDecodeError):\n",
    "        logs = []\n",
    "    logs.append(log_entry)\n",
    "    with open(log_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(logs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞ –≤–µ—Ä—Å–∏–π\n",
    "def update_version_log():\n",
    "    version_entry = {\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"files\": [\n",
    "            {\n",
    "                \"file\": \"rag_article.jsonl\",\n",
    "                \"description\": \"–ò—Å—Ö–æ–¥–Ω—ã–π —Ñ–∞–π–ª —Å —á–∞–Ω–∫–∞–º–∏ —Å—Ç–∞—Ç–µ–π –¥–ª—è RAG.\"\n",
    "            },\n",
    "            {\n",
    "                \"file\": \"embeddings.pkl\",\n",
    "                \"description\": \"–°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–∞–Ω–∫–æ–≤ –¥–ª—è Qdrant.\"\n",
    "            },\n",
    "            {\n",
    "                \"file\": \"rag_result.json\",\n",
    "                \"description\": \"–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (–≤–æ–ø—Ä–æ—Å, –æ—Ç–≤–µ—Ç, —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏).\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    os.makedirs('artifacts', exist_ok=True)\n",
    "    with open('artifacts/version_log.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump([version_entry], f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "# –ó–∞–ø—Ä–æ—Å\n",
    "query = \"–ß—Ç–æ –∞–≤—Ç–æ—Ä –∏–º–µ–µ—Ç –≤ –≤–∏–¥—É –ø–æ–¥ '–∏–ª–ª—é–∑–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞'?\"\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ –∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5001\")  # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ MLflow\n",
    "with mlflow.start_run():  # –ù–∞—á–∞—Ç—å MLflow run\n",
    "    mlflow.log_param(\"query\", query)  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞\n",
    "    relevant_chunks = search_chunks(query, top_k=3, min_score=0.4)\n",
    "    answer = generate_response(query, relevant_chunks)\n",
    "    mlflow.log_param(\"num_chunks\", len(relevant_chunks))  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —á–∏—Å–ª–∞ —á–∞–Ω–∫–æ–≤\n",
    "    mlflow.log_artifact(\"artifacts/rag_result.json\")  # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞\n",
    "\n",
    "# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "result = {\"query\": query, \"answer\": answer, \"relevant_chunks\": relevant_chunks}\n",
    "os.makedirs('artifacts', exist_ok=True)\n",
    "with open('artifacts/rag_result.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞\n",
    "log_query(query, answer, relevant_chunks)\n",
    "\n",
    "# –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∂—É—Ä–Ω–∞–ª–∞ –≤–µ—Ä—Å–∏–π\n",
    "update_version_log()\n",
    "\n",
    "# –í—ã–≤–æ–¥\n",
    "print(f\"\\n–í–æ–ø—Ä–æ—Å: {query}\")\n",
    "print(f\"–û—Ç–≤–µ—Ç: {answer}\")\n",
    "print(\"–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏:\")\n",
    "for i, chunk in enumerate(relevant_chunks, 1):\n",
    "    print(f\"–ß–∞–Ω–∫ {i}: {chunk[:100]}...\" if chunk else f\"–ß–∞–Ω–∫ {i}: –ü—É—Å—Ç–æ\")"
   ],
   "id": "deca4b09975ae2e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run bright-wolf-353 at: http://localhost:5001/#/experiments/0/runs/29326a205e5a40aa9954dc43e453aa2b\n",
      "üß™ View experiment at: http://localhost:5001/#/experiments/0\n",
      "\n",
      "–í–æ–ø—Ä–æ—Å: –ß—Ç–æ –∞–≤—Ç–æ—Ä –∏–º–µ–µ—Ç –≤ –≤–∏–¥—É –ø–æ–¥ '–∏–ª–ª—é–∑–∏–µ–π –∫–∞—á–µ—Å—Ç–≤–∞'?\n",
      "–û—Ç–≤–µ—Ç: –í–Ω—É—Ç—Ä–∏ ‚Äî —è–¥—Ä–æ: –ª–æ–≥–∏–∫–∞, —Å–º—ã—Å–ª, —Ä–µ–∑—É–ª—å—Ç–∞—Ç. –ò–ª–ª—é–∑–∏—è –ö–∞—á–µ—Å—Ç–≤–∞ ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –≤—ã –ø–æ–∫—É–ø–∞—Ç–µ —à–µ–ª—É—Ö—É, –ø–µ—Ä–µ–ø—É—Ç–∞–≤ –µ–µ —Å —Å–µ—Ä–¥—Ü–µ–≤–∏–Ω–æ–π. –í—ã —Å–º–æ—Ç—Ä–∏—Ç–µ –Ω–∞ —Å–∞–π—Ç, –∫–∞–∫ –Ω–∞ –≤–∏—Ç—Ä–∏–Ω—É ‚Äî –∏ –≤–µ—Ä–∏—Ç–µ, —á—Ç–æ —ç—Ç–æ –±–∏–∑–Ω–µ—Å-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç. –ê –Ω–∞ —Å–∞–º–æ–º –¥–µ–ª–µ ‚Äî –ø—Ä–æ—Å—Ç–æ –¥–µ–∫–æ—Ä.\n",
      "–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —á–∞–Ω–∫–∏:\n",
      "–ß–∞–Ω–∫ 1: —Ä–∞—Å—Å–µ–∫–∞–µ–º ‚Äú–ö–∞—á–µ—Å—Ç–≤–æ‚Äù –ò–ª–ª—é–∑–∏—è –ö–∞—á–µ—Å—Ç–≤–∞ ‚Äî —ç—Ç–æ –∫–æ–≥–¥–∞ –≤—ã –ø–ª–∞—Ç–∏—Ç–µ –∑–∞ –ª–æ—Å–∫, –∞ –ø–æ–ª—É—á–∞–µ—Ç–µ –ø—É—Å—Ç–æ—Ç—É –ö–∞—á–µ—Å—Ç–≤–æ —Ü...\n",
      "–ß–∞–Ω–∫ 2: –∞–Ω–æ–Ω–∏–º–Ω—ã—Ö –ø–µ—Ä—Ñ–µ–∫—Ü–∏–æ–Ω–∏—Å—Ç–æ–≤ –ö—Å—Ç–∞—Ç–∏, –æ ‚Äú–ø–∞–º—è—Ç–Ω–∏–∫–∞—Ö‚Äù. –ö–∞–∫–æ–π —Å–∞–º—ã–π –≤–æ–ø–∏—é—â–∏–π –ø—Ä–∏–º–µ—Ä ‚Äú–ò–ª–ª—é–∑–∏–∏ –∫–∞—á–µ—Å—Ç–≤–∞‚Äù –≤—ã ...\n",
      "–ß–∞–Ω–∫ 3: 1: –ü–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ (–í–∏—Ç—Ä–∏–Ω–∞) –ß—Ç–æ —ç—Ç–æ? –í—Å—ë, —á—Ç–æ –º–æ–∂–Ω–æ –æ—Ü–µ–Ω–∏—Ç—å –∑–∞ 5 —Å–µ–∫—É–Ω–¥, –Ω–µ –≤–Ω–∏–∫–∞—è: –ø–∏–∫—Å–µ–ª—å-...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7435cbd788c6e4ac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4495d129236a61ae"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
